---
title: "Student Data Analysis"
author: "Your Name"
date: "`r Sys.Date()`"
format: html
self-contained: true
---

```{r}
# Your R code will go here
library(here)
library(tidyverse)
df <- read_csv(here('./chap_03/data/ student_data.csv'))
```

```{r}
# View the data structure
df |> 
    View()
```

Determine if there are any missing data in each of the variables.

```{r}
# Count missing values in each column
missing_counts <- df |> 
    map_dbl(~sum(is.na(.)))

# Display the missing value counts
print(missing_counts)
```

```{r}
# Alternative approach with a more informative output
missing_summary <- df |> 
    summarise(across(everything(), ~sum(is.na(.)))) |> 
    pivot_longer(everything(), names_to = "variable", values_to = "missing_count")

# Display the missing value summary
print(missing_summary)
```

```{r}
# Visualize missing values
# library(ggplot2)

missing_summary |> 
    ggplot(aes(x = reorder(variable, -missing_count), y = missing_count)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    theme_minimal() +
    labs(title = "Missing Values by Variable",
         x = "Variable",
         y = "Number of Missing Values") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Next, we convert several variables—noise_level_pre, noise_level_post, and part_time_job—into factors to properly represent categorical data. 

```{r}
df <- df |> 
    mutate(across(c(noise_level_pre, noise_level_post, part_time_job), ~as.factor(.))) 
```

Finally, we calculate a new variable, grade_improvement, by subtracting the pre-move grade from the post-move grade.


```{r}
df <- df |> 
    mutate(grade_improvement = post_move_grade - pre_move_grade)
```

Examine pre move variables

```{r}
df |> 
    select(contains("pre")) |> 
    names()
```

Pre-move variables are `pre_move_grade` and `noise_level_pre`

```{r}
df |> 
    ggplot(aes(x=noise_level_pre, y=pre_move_grade)) +
    geom_boxplot() +
    geom_jitter(width = 0.2, alpha=0.3) +
    theme_bw() +
    labs(title = "Pre move grade vs pre move noise level",
    x = "Pre move noise level", y = "Pre move grade")
```

Examine relationship between hours studied and grade improvement


```{r}
df |> 
    ggplot(aes(x=study_hours, y=grade_improvement)) +
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    theme_minimal() +
    labs(title = "Hours studied vs Grade Improvement",
    x = "Hours studied", y = "Grade improvement")
```


```{r}
df |> 
    summarise(mean=mean(pre_move_grade), sd=sd(pre_move_grade), .by=noise_level_pre)
```

## Correlation Analysis

```{r}
# Install and load required packages
if (!require("corrplot")) install.packages("corrplot")
library(corrplot)

# Identify numeric variables in the dataset
numeric_vars <- df |> 
    select(where(is.numeric)) |> 
    names()

print("Numeric variables in the dataset:")
print(numeric_vars)
```

```{r}
# Calculate correlation matrix
cor_matrix <- df |> 
    select(where(is.numeric)) |> 
    cor(use = "complete.obs", method = "pearson")

print("Correlation Matrix:")
print(round(cor_matrix, 2))
```

```{r}
# Create correlation heatmap
corrplot(cor_matrix, 
         method = "color", 
         type = "upper", 
         tl.col = "black", 
         tl.srt = 45,
         addCoef.col = "black",
         number.cex = 0.7,
         title = "Correlation Matrix Heatmap",
         mar = c(0,0,1,0))
```

```{r}
# Alternative visualization with ggplot2
library(reshape2)

cor_melted <- melt(cor_matrix)

ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
    geom_tile() +
    geom_text(aes(label = round(value, 2)), color = "black", size = 3) +
    scale_fill_gradient2(low = "blue", mid = "white", high = "red", 
                         midpoint = 0, limit = c(-1, 1)) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          panel.grid = element_blank()) +
    labs(title = "Correlation Matrix Heatmap",
         x = "",
         y = "",
         fill = "Correlation")
```

```{r}
# Scatterplot matrix for key variables
library(GGally)

# Select key numeric variables for scatterplot matrix
key_vars <- c("pre_move_grade", "post_move_grade", "study_hours", "grade_improvement")

ggpairs(df[, key_vars], 
        upper = list(continuous = wrap("cor", size = 4)),
        lower = list(continuous = wrap("points", alpha = 0.3, size = 0.5)),
        diag = list(continuous = wrap("densityDiag", alpha = 0.5)),
        title = "Scatterplot Matrix of Key Variables")
```

## Simple Causal Inference

```{r}
t.test(df$pre_move_grade, df$post_move_grade, paired = TRUE)
```

::: {.callout-note}
## Paired t-test Interpretation (gpt5-mini)

**Brief interpretation**

- Test: paired t-test on 100 paired observations (df = 99).
- Result: t = -0.387, p = 0.6996. Because p > 0.05, you fail to reject the null hypothesis that the mean difference = 0. There is no evidence of a statistically significant difference between the paired measurements.
- Estimate: the observed mean difference = -0.6266 (on average the first measurement is 0.6266 units lower than the second).
- Confidence interval: 95% CI = [−3.840, 2.587]. This interval includes 0, consistent with the non-significant p-value; the true mean difference could plausibly be anywhere in that range.

**Notes/implications**
- "Failing to reject" is not proof of no difference. If you need to demonstrate practical equivalence, use an equivalence (TOST) procedure or ensure adequate power/smaller CI width.
- The paired t-test assumes the pairwise differences are approximately normally distributed; check this assumption (e.g., histogram, QQ-plot, Shapiro–Wilk) if unsure.
:::


```{r}
df |> 
    ggplot(aes(x=grade_improvement)) +
    geom_density() +
    theme_minimal()
```


```{r}
shapiro.test(df$grade_improvement)
```

::: {.callout-note}
## Shapiro-Wilk test interpretation (gpt5-mini)
Brief answer
- Null hypothesis (H0): the data are drawn from a normal distribution.
- Result: W = 0.97826, p = 0.09715.
- Interpretation at the common α = 0.05: p > 0.05, so you fail to reject H0 — there is no strong evidence the data deviate from normality (i.e., the data are consistent with normality).
- Note: at α = 0.10 the result is borderline (p ≈ 0.097), so some might treat it as marginal evidence against normality.

Extra context and advice
- The W statistic ranges 0–1; values closer to 1 indicate closer agreement with a normal distribution. W = 0.978 is fairly close to 1.
- "Fail to reject" does not prove normality; it only means the test did not find significant departure given your sample size.
- Shapiro-Wilk is sensitive to sample size: with very large samples tiny departures can be significant; with small samples the test may lack power to detect departures.
- Recommended checks: plot a Q–Q plot and a histogram, compute skewness/kurtosis, and consider robustness or transformations if the analysis is sensitive to normality.
:::

## Linear Regression Analysis

```{r}
# Fit the linear regression model
lm_model <- lm(grade_improvement ~ noise_level_pre + noise_level_post + 
               study_hours + part_time_job + family_income, data = df)

# Display the model summary
summary(lm_model)
```

```{r}
# Visual interpretation of regression results
# 1. Residual plots for model assumptions
par(mfrow = c(2, 2))
plot(lm_model, which = 1:4)
par(mfrow = c(1, 1))
```

```{r}
# 2. Coefficient plot with confidence intervals
library(broom)
library(ggplot2)

# Extract coefficients and confidence intervals
coef_data <- tidy(lm_model, conf.int = TRUE) |> 
    filter(term != "(Intercept)")

# Create coefficient plot
ggplot(coef_data, aes(x = reorder(term, estimate), y = estimate, ymin = conf.low, ymax = conf.high)) +
    geom_pointrange(color = "blue", size = 0.7) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    coord_flip() +
    theme_minimal() +
    labs(title = "Regression Coefficients with 95% Confidence Intervals",
         x = "Predictor Variables",
         y = "Coefficient Estimate") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
# 3. Check for influential observations
influence_data <- influence.measures(lm_model)

# Cook's distance plot
plot(lm_model, which = 4, main = "Cook's Distance")
```

```{r}
# 4. Check multicollinearity
library(car)

# Calculate VIF (Variance Inflation Factor)
vif_results <- vif(lm_model)
print("Variance Inflation Factors (VIF):")
print(vif_results)

# Check correlation between predictors
predictor_cor <- cor(df |> 
                        select(where(is.numeric), -grade_improvement))
print("Correlation matrix of predictors:")
print(round(predictor_cor, 2))
```

::: {.callout-warning}
## Linear Regression Model Interpretation

**Model Overview**
- Dependent variable: `grade_improvement` (post_move_grade - pre_move_grade)
- Predictors: noise levels (pre/post), study hours, part-time job status, family income
- Sample size: 100 observations

**Overall Model Fit**
- **R-squared**: 0.076 (7.6%) - The model explains only 7.6% of the variance in grade improvement
- **Adjusted R-squared**: 0.027 (2.7%) - Very low explanatory power even after adjusting for predictors
- **F-statistic**: 1.556 (p = 0.180) - Overall model is not statistically significant
- **Residual standard error**: 15.97 - Typical prediction error is about 16 points

**Key Findings**

**Statistically Significant Predictor:**
- **study_hours**: -1.75 (p = 0.045) - For each additional hour of study, grade improvement decreases by 1.75 points, on average. This counterintuitive result warrants investigation.

**Non-Significant Predictors:**
- `noise_level_preModerate`: p = 0.683 (no significant effect)
- `noise_level_postModerate`: p = 0.409 (no significant effect)  
- `part_time_jobYes`: p = 0.149 (marginally non-significant)
- `family_income`: p = 0.691 (no significant effect)

**Model Assumption Checks**

**Residual Analysis:**
- Residuals show reasonable distribution but with some potential outliers
- Q-Q plot suggests minor deviations from normality
- Scale-Location plot indicates potential heteroscedasticity

**Multicollinearity:**
- VIF values are all < 2, indicating no problematic multicollinearity
- Predictor correlations are generally low

**Influential Observations:**
- Cook's distance values are mostly < 0.5, suggesting no highly influential points
- A few observations may warrant closer examination

**Practical Implications**
1. **Limited Explanatory Power**: The model has very low predictive capability, suggesting other important factors influence grade improvement that are not captured in this model.

2. **Counterintuitive Finding**: The negative relationship between study hours and grade improvement is unexpected and may indicate:
   - Data quality issues
   - Omitted variable bias (e.g., motivation, prior academic preparation)
   - Measurement error in study hours
   - Non-linear relationships not captured by the model

3. **Recommendations**:
   - Consider additional relevant predictors (e.g., student motivation, teaching quality, course difficulty)
   - Explore non-linear relationships and interactions
   - Investigate data collection methods for study hours
   - Consider robust regression methods if outliers are problematic
:::

## Using Propensity Score Matching

```{r}
library(MatchIt)
m.out <- matchit(noise_level_post ~ study_hours +
                   part_time_job + family_income,
                 method = "nearest", data = df)
matched_data <- match.data(m.out)  # Getting the matched dataset
# matched_data |> View()
```


**Individual cases:** You can examine how individual students' grades changed after moving. For example, a student with a significant increase in grade_improvement after moving to a Low noise level post-move might suggest a beneficial effect of a quieter environment. For instance, Student 1, who remained in a Moderate noise level, showed a grade improvement of 12.496 points. Student 3, moving from a Moderate to Low noise level, shows a smaller improvement

```{r}
df |> 
    select(contains("noise"), contains("grade")) |> 
    mutate(pct_change = if_else( pre_move_grade - post_move_grade == 0, 0, round(grade_improvement/pre_move_grade, 2))) |> 
    group_by(noise_level_pre, noise_level_post) |> 
    summarise(mean_pct_change=mean(pct_change))
```

```{r}
df |> 
    select(contains("noise"), contains("grade")) |> 
    mutate(pct_change = if_else( pre_move_grade - post_move_grade == 0, 0, round(grade_improvement/pre_move_grade, 2))) |> 
    ggplot(aes(x=noise_level_post, y = pct_change)) +
    geom_boxplot() +
    facet_wrap(~noise_level_pre)
```

## Interpreting MatchIt Output: Distance, Weights, and Subclass

The MatchIt procedure produces several important columns that help us understand the matching process:

### Key Columns Explained:

1. **Distance**: The propensity score (estimated probability of being in "High" noise level group given covariates)
2. **Weights**: Sampling weights that adjust for treatment assignment probability (1 = successfully matched, 0 = not matched)
3. **Subclass**: Indicates which matched pair/group each observation belongs to

### Visualizations for Better Understanding

```{r}
# Load necessary packages for visualization
library(ggplot2)
library(dplyr)
library(cobalt)
```

```{r}
# 1. Propensity Score Distribution Plot
matched_data %>%
  mutate(noise_level_post = as.factor(noise_level_post)) %>%
  ggplot(aes(x = distance, fill = noise_level_post)) +
  geom_density(alpha = 0.6) +
  theme_minimal() +
  labs(title = "Propensity Score Distribution by Post-Move Noise Level",
       x = "Propensity Score (Distance)",
       y = "Density",
       fill = "Post-Move Noise Level") +
  theme(plot.title = element_text(hjust = 0.5))
```

This plot shows how well the treatment and control groups overlap in their propensity scores. Good overlap indicates that matching was successful.

```{r}
# 2. Matching Visualization - Show matched pairs by subclass
matched_data %>%
  group_by(subclass) %>%
  summarise(
    treated = sum(noise_level_post == "High", na.rm = TRUE),
    control = sum(noise_level_post == "Low", na.rm = TRUE),
    .groups = "drop"
  ) %>%
  ggplot(aes(x = factor(subclass), y = treated, fill = "Treated")) +
  geom_col(position = "dodge") +
  geom_col(aes(y = control, fill = "Control"), position = "dodge") +
  theme_minimal() +
  labs(title = "Matched Pairs by Subclass",
       x = "Subclass",
       y = "Number of Students",
       fill = "Group") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(hjust = 0.5))
```

This visualization shows the structure of your matched pairs. Each subclass represents a matched group containing both treated and control students.

```{r}
# 3. Weight Distribution Plot
matched_data %>%
  ggplot(aes(x = weights, fill = noise_level_post)) +
  geom_histogram(bins = 20, alpha = 0.7, position = "identity") +
  theme_minimal() +
  labs(title = "Distribution of Matching Weights",
       x = "Weights",
       y = "Frequency",
       fill = "Post-Move Noise Level") +
  theme(plot.title = element_text(hjust = 0.5))
```

Since all your weights are 1, this confirms that all observations were successfully matched in the nearest neighbor matching process.

```{r}
# 4. Balance Assessment Plot
# Check balance before and after matching
bal <- bal.tab(m.out, stats = c("mean.diffs", "variance.ratios"))
love.plot(bal, stars = "raw", main = "Balance Assessment Before and After Matching")
```

This Love plot is crucial for assessing whether matching successfully balanced the covariates between treatment and control groups. Smaller differences (closer to 0) indicate better balance.

```{r}
# 5. Distance vs. Outcome Plot
matched_data %>%
  ggplot(aes(x = distance, y = grade_improvement, color = noise_level_post)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  labs(title = "Propensity Score vs. Grade Improvement",
       x = "Propensity Score (Distance)",
       y = "Grade Improvement",
       color = "Post-Move Noise Level") +
  theme(plot.title = element_text(hjust = 0.5))
```

This plot helps you see if there's any relationship between the propensity score and the outcome variable, which can indicate potential issues with the matching or model specification.

### Summary of Matching Results

Based on the visualizations above, you can assess:

1. **Overlap**: How well do the propensity score distributions overlap between groups?
2. **Balance**: Did matching successfully balance the covariates?
3. **Structure**: How many matched pairs do you have, and what's their composition?
4. **Weight distribution**: Are all observations properly weighted?

These visualizations make the abstract concepts of distance, weights, and subclass more concrete and interpretable.
